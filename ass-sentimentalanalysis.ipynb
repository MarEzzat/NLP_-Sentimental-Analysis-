{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load Dataset\ncolumns = ['id', 'country', 'Label', 'Text']\ndf = pd.read_csv(\"/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv\", names=columns)\ndf.dropna(inplace=True)\n\n# Encode Labels\nle = LabelEncoder()\ndf['Label'] = le.fit_transform(df['Label'])\n\n# Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Text'])\nvocab_size = len(tokenizer.word_index) + 1\nsequences = tokenizer.texts_to_sequences(df['Text'])\nmax_length = max(len(seq) for seq in sequences)\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['Label'], test_size=0.2, random_state=42, stratify=df['Label'])\n\n# Model Architecture\nmodel = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=128),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Dropout(0.5),\n    BatchNormalization(),\n    Bidirectional(LSTM(32)),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\n# Compile Model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n\n# Learning Rate Scheduler\ncallback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_lr=1e-5)\n\n# Train Model\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=[callback])\n\n# Evaluate Model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Classification Report\nfrom sklearn.metrics import classification_report\ny_pred = np.argmax(model.predict(X_test), axis=1)\nprint(classification_report(y_test, y_pred, target_names=le.classes_)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T15:28:39.528008Z","iopub.execute_input":"2025-03-21T15:28:39.528318Z","iopub.status.idle":"2025-03-21T15:35:31.631650Z","shell.execute_reply.started":"2025-03-21T15:28:39.528296Z","shell.execute_reply":"2025-03-21T15:35:31.630756Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 43ms/step - accuracy: 0.4578 - loss: 1.1923 - val_accuracy: 0.6689 - val_loss: 0.8931 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.8228 - loss: 0.5280 - val_accuracy: 0.8501 - val_loss: 0.4302 - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.8996 - loss: 0.2968 - val_accuracy: 0.8711 - val_loss: 0.3729 - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9230 - loss: 0.2213 - val_accuracy: 0.8749 - val_loss: 0.3880 - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9313 - loss: 0.1896\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9313 - loss: 0.1896 - val_accuracy: 0.8852 - val_loss: 0.3743 - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9446 - loss: 0.1509 - val_accuracy: 0.8936 - val_loss: 0.3873 - learning_rate: 5.0000e-04\nEpoch 7/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9530 - loss: 0.1231\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9530 - loss: 0.1231 - val_accuracy: 0.8955 - val_loss: 0.3970 - learning_rate: 5.0000e-04\nEpoch 8/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9589 - loss: 0.1055 - val_accuracy: 0.8980 - val_loss: 0.4271 - learning_rate: 2.5000e-04\nEpoch 9/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9622 - loss: 0.0940\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9622 - loss: 0.0940 - val_accuracy: 0.8990 - val_loss: 0.4614 - learning_rate: 2.5000e-04\nEpoch 10/10\n\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.9636 - loss: 0.0886 - val_accuracy: 0.8993 - val_loss: 0.4770 - learning_rate: 1.2500e-04\n\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9027 - loss: 0.4429\nTest Accuracy: 0.8993\n\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step\n              precision    recall  f1-score   support\n\n  Irrelevant       0.92      0.86      0.89      2575\n    Negative       0.90      0.92      0.91      4472\n     Neutral       0.90      0.90      0.90      3622\n    Positive       0.89      0.90      0.89      4131\n\n    accuracy                           0.90     14800\n   macro avg       0.90      0.90      0.90     14800\nweighted avg       0.90      0.90      0.90     14800\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}